{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models\n",
    "!mkdir plots\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassMatthewsCorrCoef\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from classifier import NappaSleepNet\n",
    "\n",
    "from utils.dataset_classes import NappaDataset\n",
    "from utils.dataset_preprocess import HybridScaler\n",
    "\n",
    "from utils.plots import plot_learning_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=4, suppress=True)\n",
    "torch.set_printoptions(precision=4,threshold=1000, sci_mode=False)\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "\n",
    "sns.set_theme(context='notebook', style='white',\n",
    "               palette='deep', font='arial',\n",
    "               font_scale=1, color_codes=True, rc=None)\n",
    "\n",
    "sns.set_style(\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device(eval=False):\n",
    "  if torch.cuda.is_available() and not eval:\n",
    "    device = torch.device('cuda')\n",
    "  else:\n",
    "    device = torch.device('cpu')\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = select_device()\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "scaler = HybridScaler(method='global')\n",
    "\n",
    "mapping = {\n",
    "    'N3'  :0,\n",
    "    'N2'  :0,\n",
    "    'N1'  :1,\n",
    "    'REM' :1,\n",
    "    'Wake':2,        \n",
    "    }\n",
    "\n",
    "nappa_dataset = NappaDataset('nappa_dataset.pkl').labelsToNumeric(mapping).sortById()\n",
    "\n",
    "sleep_classes = ['N2/N3', 'N1/REM', 'Wake']\n",
    "\n",
    "NUM_FEATURES = nappa_dataset.features.shape[1]\n",
    "NUM_CLASSES = len(sleep_classes)\n",
    "PADDING_VALUE = -1\n",
    "\n",
    "learning_rate = 1e-3\n",
    "batch_size = 4\n",
    "num_epochs = 100\n",
    "\n",
    "model = NappaSleepNet(n_features=NUM_FEATURES, n_classes=NUM_CLASSES).to(device)\n",
    "\n",
    "multiclass_matthewscorrcoef = MulticlassMatthewsCorrCoef(num_classes = NUM_CLASSES, ignore_index=PADDING_VALUE).to(device)\n",
    "multiclass_accuracy = MulticlassAccuracy(num_classes = NUM_CLASSES, average='micro', ignore_index=PADDING_VALUE).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(model, criterion, dataloader):\n",
    "    \"\"\"\n",
    "    Computes the loss, accuracy, and Matthews correlation coefficient (MCC) for the given model and data.\n",
    "\n",
    "    This function iterates over the provided data loader to accumulate the model's loss and compute\n",
    "    the accuracy and MCC for the predictions. The metrics are computed for the entire dataset\n",
    "    aggregated across all batches in the data loader.\n",
    "\n",
    "    Args:\n",
    "        model: The neural network model to evaluate.\n",
    "        criterion: The loss function used to evaluate the model's performance.\n",
    "        dataloader: The data loader containing the dataset to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the average loss, overall accuracy, and overall MCC for the dataset.\n",
    "    \"\"\"\n",
    "    total_loss = 0.0\n",
    "    multiclass_accuracy.reset()\n",
    "    multiclass_matthewscorrcoef.reset()\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features, labels, rec_lengths in dataloader:\n",
    "\n",
    "          features = features.to(device).type(torch.float)\n",
    "          labels = labels.to(device).type(torch.long)\n",
    "\n",
    "          output = model(features, rec_lengths)\n",
    "          loss = criterion(output.reshape(-1, NUM_CLASSES), labels.flatten())\n",
    "          total_loss += loss.item()\n",
    "\n",
    "          predicted_classes = torch.argmax(output, dim=-1)\n",
    "\n",
    "          mcc = multiclass_matthewscorrcoef(predicted_classes, labels)\n",
    "          accuracy = multiclass_accuracy(predicted_classes, labels)\n",
    "\n",
    "\n",
    "    # Aggregate the results from all batches to get metrics on the whole input dataset\n",
    "    accuracy = multiclass_accuracy.compute()\n",
    "    mcc = multiclass_matthewscorrcoef.compute()\n",
    "\n",
    "    return total_loss / len(dataloader), accuracy.cpu(), mcc.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate(batch):\n",
    "  \"\"\"\n",
    "  Prepares a batch of sleep recordings for training by padding them to equal lengths.\n",
    "\n",
    "  This function takes a list of SleepRecording instances and pads their features and labels \n",
    "  to ensure that each sequence in the batch has the same length.\n",
    "  Args:\n",
    "    batch (list): A list of SleepRecording instances.\n",
    "\n",
    "  Returns:\n",
    "    tuple: A tuple containing the following elements:\n",
    "      - features_padded: A tensor of padded features with shape \n",
    "        (batch_size, max_rec_length, n_features), where max_rec_length is the length of \n",
    "        the longest recording in the batch.\n",
    "      - labels_padded: A tensor of padded labels with shape \n",
    "        (batch_size, max_rec_length), corresponding to the padded features.\n",
    "      - lengths (list): A list of the original lengths of each recording in the batch.\n",
    "  \"\"\"\n",
    "  # Extract the lengths of each recording and sort the batch based on these lengths\n",
    "  rec_lengths = [len(recording.features) for recording in batch]\n",
    "  sorted_indices = sorted(range(len(rec_lengths)), key=rec_lengths.__getitem__, reverse=True)\n",
    "  batch = [batch[i] for i in sorted_indices]\n",
    "\n",
    "  # Convert features and labels of each recording to tensors\n",
    "  features = [torch.tensor(recording.features) for recording in batch]\n",
    "  labels = [torch.tensor(recording.labels) for recording in batch]\n",
    "\n",
    "  # Pad features and labels to the length of the longest recording in the batch\n",
    "  features_padded = pad_sequence(features, batch_first=True, padding_value= PADDING_VALUE)\n",
    "  labels_padded = pad_sequence(labels, batch_first=True, padding_value= PADDING_VALUE)\n",
    "\n",
    "  # Return the padded features and labels, along with the sorted lengths\n",
    "  return features_padded, labels_padded, sorted(rec_lengths, reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LOSOCV(model, dataset, lr, batch_size, num_epochs, verbose=False, plot_curves=False):\n",
    "\n",
    "  \"\"\"\n",
    "  Performs Leave-One-Subject-Out Cross-Validation (LOSOCV) on a given dataset using given model.\n",
    "\n",
    "  Args:\n",
    "      model: The neural network model to be trained and evaluated.\n",
    "      dataset: The dataset containing all subjects' data, each subject will be used once as a test set.\n",
    "      lr: Learning rate for the optimizer.\n",
    "      batch_size: Number of sleep recordings per batch.\n",
    "      num_epochs: Number of epochs to train each model.\n",
    "      verbose: If True, prints detailed logs for each fold.\n",
    "      plot_curves: If True, generates and shows learning curves\n",
    "  \"\"\"\n",
    "  all_train_metrics = np.zeros((3, len(dataset), num_epochs))\n",
    "  all_test_metrics = np.zeros((3, len(dataset), num_epochs))\n",
    "\n",
    "  for fold, test_subject in enumerate(dataset):\n",
    "\n",
    "      # Exclude the current test subject to create the training dataset\n",
    "      train_dataset = NappaDataset([subject for subject in dataset if subject.id != test_subject.id])\n",
    "\n",
    "      # Calculate global mean and standard deviation for feature normalization\n",
    "      train_global_mean = train_dataset.features.mean(axis=0)\n",
    "      train_global_std = train_dataset.features.std(axis=0)\n",
    "\n",
    "      # Normalize the features of sleep recordings in the training set\n",
    "      train_dataset = scaler(train_dataset)\n",
    "\n",
    "      # Construct the test set. For global normalization,\n",
    "      # normalize the data using training set mean and std.\n",
    "      test_dataset = scaler(NappaDataset([test_subject]), is_testset=True,\n",
    "                                                    trainset_mean=train_global_mean,\n",
    "                                                    trainset_std=train_global_std)\n",
    "\n",
    "      # Create dataloaders for both datasets. Shuffle the training data for each epoch randomly.\n",
    "      train_loader = DataLoader(train_dataset, collate_fn=collate, batch_size=batch_size, shuffle=True)\n",
    "      test_loader = DataLoader(test_dataset, collate_fn=collate, batch_size=1)\n",
    "\n",
    "      criterion = nn.CrossEntropyLoss(ignore_index=PADDING_VALUE)\n",
    "\n",
    "      model.reset() # reset model parameters for each test subject/fold\n",
    "      optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "      if verbose:\n",
    "        print(f'----------------------fold number {fold+1}----------------------')\n",
    "        print(f'Test subject id: {test_subject.id}, age: {test_subject.age} months')\n",
    "        print(f'Train set ids: {train_dataset.ids}')\n",
    "        print(f'Test set size: {test_dataset.labels.shape[0]}')\n",
    "        print(f'Train set size: {train_dataset.labels.shape[0]}\\n')\n",
    "\n",
    "      for epoch in range(num_epochs):\n",
    "\n",
    "        model.train()\n",
    "        for features, labels, rec_lengths in train_loader:\n",
    "\n",
    "          features = features.to(device).type(torch.float)\n",
    "          labels = labels.to(device).type(torch.long)\n",
    "\n",
    "          optimizer.zero_grad()\n",
    "\n",
    "          output = model(features, rec_lengths).reshape(-1, NUM_CLASSES)\n",
    "          loss = criterion(output, labels.flatten())\n",
    "\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "        train_metrics = compute_metrics(model, criterion, train_loader)\n",
    "        test_metrics = compute_metrics(model, criterion, test_loader)\n",
    "\n",
    "        for i, (train_metric, test_metric) in enumerate(zip(train_metrics, test_metrics)):\n",
    "            all_train_metrics[i, fold, epoch] = train_metric\n",
    "            all_test_metrics[i, fold, epoch] = test_metric\n",
    "\n",
    "        if verbose and ((epoch + 1) % 20 == 0 or (epoch + 1 == num_epochs or epoch==0)):\n",
    "          print(f'Epoch {(epoch + 1):<5} {\"Test set\":10} {\"Training set\":5}')\n",
    "          print(f'{\"MCC:\":<12} {test_metrics[2]:<7.2f} {train_metrics[2]:8.2f}')\n",
    "          print(f'{\"Accuracy:\":<12} {test_metrics[1]:<7.1%} {train_metrics[1]:9.1%}')\n",
    "          print(f'{\"Loss:\":<12} {test_metrics[0]:<7.2f} {train_metrics[0]:8.2f}')\n",
    "          print()\n",
    "\n",
    "      if plot_curves:\n",
    "        fig = plot_learning_curves(all_train_metrics, all_test_metrics, num_epochs, fold, final_plot=False)\n",
    "        plt.show(fig)\n",
    "        plt.close(fig)\n",
    "\n",
    "      # Save the model weights for each test subject\n",
    "      torch.save(model.state_dict(), f'models/model_subject_{test_subject.id}.pth')\n",
    "      if verbose:\n",
    "        print(f'Model saved to: models/model_subject_{test_subject.id}.pth \\n')\n",
    "\n",
    "  if plot_curves:\n",
    "    fig = plot_learning_curves(all_train_metrics, all_test_metrics, num_epochs, fold, final_plot=True)\n",
    "    fig.savefig(f'plots/learning curves.png')\n",
    "    plt.show(fig)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSOCV(model, nappa_dataset, lr=learning_rate, batch_size=batch_size,\n",
    "       num_epochs = num_epochs, verbose=True, plot_curves=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
